---
title: "Artificial data analysis"
header-includes:
    - \usepackage{bm}
output:
  rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{artificial_data_analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.width = 6,
  fig.height = 4
)
```

```{r setup}
library(papss)
library(mgcv)
set.seed(2022) # For replicability.
```

## Introduction

Hoeks & Levelt (1993) investigated the characteristics of the function relating an increase in 'cognitive demand' to a change in the dilation of the pupil. They briefly summarize the extensive body of research relating changes in pupil dilation to the effort associated with cognitive processing - but Kahneman's book 'Attention and Effort' already published in 1973 might provide a better overview for the interested reader. Hoeks & Levelt were motivated by the fact that the pupil dilation time-course is best described as a slowly evolving signal, which obscures how changes in demand exactly contributed to its shape. They revealed that the increase in dilation following a spike in demand is best modeled using a Gamma-Erlang function. They also determined the optimal values for the parameters associated with the latter experimentally. You can check the documentation of the *h_basis* function if you want to know more (or read their paper!):

```{r, eval=F, echo=T}
help(h_basis)
```

In their original work they really focused on isolated 'demand spikes' linked to clearly identifiable events in their experiment setup. Below we visualize their 'pupil-response function':

```{r}
# First we create a time variable.
time <- seq(0,2500,by=20) # Time at 50 HZ

# Index vector for possible pulse locations, pointing to each sample
pulse_locations <- seq(1,length(time),by=1)

# Now we can calculate the effect on the pupil of an isolated spike
# that happened during the 5th. sample.
# Note that the 'f' parameter was added later by Wierda et al. (2012) to
# scale the response of the pupil to a spike of fixed strength.
pupil_dilation <- h_basis(5,time,pulse_locations,n=10.1,t_max=930,f=1/(10^24))

# Which we can now visualize...
plot(time,pupil_dilation,typ="l",lwd=3,xlab="time",ylab="Pupil dilation")

# Together with the spike...
abline(v=time[5],lty=2,lwd=3,col="red")

legend("topright",
       c("Pupil response","Spike in demand"),
       lty = c(1,2),lwd=3,col=c("black","red"))
```

This figure exemplifies what Hoeks & Levelt meant when they said that the pupil dilation changes slowly with time. It also highlights the contribution of the 't_max' parameter, since the pupil response peaks around 930 ms after the spike occurred.

According to their analysis, the **pupil dilation time-course** we end up observing then is nothing more than the accumulation of multiple **weighted** spikes (assuming that external conditions such as illumination and gaze direction remain stationary) - each having an additive effect on the dilation of the pupil:

```{r}
# Now lets add some more spikes - and vary their weight!

# This variable will hold the sum of the individual responses - i.e., what we could observe!
pupil <- rep(0,length.out=length(time))

# We now refer to the resulting variables as h_{i}, the purpose will become
# clear in the new paragraph.
H <- matrix(nrow=length(time),ncol = 4)
h1 <- h_basis(5,time,pulse_locations,n=10.1,t_max=930,f=1/(10^24))
H[,1] <- h1
# Don't forget to add the weighted contribution of this spike to the overall time-course
pupil <- pupil + 0.3 * h1

# Visualization of the individual contribution
plot(time,0.3 * h1,typ="l",
     lwd=3,xlab="time",
     ylab="Pupil dilation",
     ylim=c(0,70),lty=2)

# Together with the spike...
abline(v=time[5],lty=2,lwd=3,col="black")

# Sample some weights
betas <- runif(3)

# Now add more spikes!
iw <- 1
for (si in c(10,15,20)) {
  hi <- h_basis((si*3),time,pulse_locations,n=10.1,t_max=930,f=1/(10^24))
  pupil <- pupil + betas[iw] * hi
  H[,(iw+1)] <- hi
  
  lines(time,betas[iw] * hi,lwd=3,lty=2,col=si)
  abline(v=time[(si*3)],lty=2,lwd=3,col=si)
  iw <- iw +1
}

# And the resulting pupil dilation time-course we could actually observe!
lines(time,pupil,lwd=3)

# Merge weights for later
betas <- c(0.3,betas)
```

The weight of these spikes is of course only selected at random here for illustrative purposes. In practice, what researchers are interested in is to find the spike weights that most likely generated an observed pupil dilation time-course. Hoeks & Levelt recommended that this can be solved by means of least squares - which in the end boils down to solving an **additive model of the pupil size**. For our toy example with 4 spikes, this would look like this:

\begin{align}
pupil_i = \hat{\beta_{1}} * h_{1}(time_{i}) + \hat{\beta_{2}} * h_{2}(time_{i}) + \hat{\beta_{3}} * h_{3}(time_{i}) + \hat{\beta_{4}} * h_{4}(time_{i}) + \epsilon_i
\end{align}

or more conveniently:

\begin{align}
pupil_{i} = \sum_{k=1}^{4} \hat{\beta_{k}} * h_{k}(time_{i}) + \epsilon_i
\end{align}

In a simple additive model, the $\epsilon_i$ are assumed to be normally distributed around zero. This might not necessarily be true for our pupil model and should be evaluated carefully.

If then $\mathbf{b}$ is a vector containing the four individual $\beta_i$ and $\mathbf{H}$ is a matrix where each of the four columns corresponds to a $h_i$, with each row corresponding to an individual $h_i({time_{i}})$ (visualized below),

```{r}
# This is the matrix H, with 4 columns, each corresponding to one of the hi
# visualized earlier.
heatmap(H,Colv = NA,Rowv=NA,scale = "none")
```


then they argue that one option to recover the values for the $\beta_i$ is to solve the following least squares optimization problem:

\begin{align}
\hat{\mathbf{b}} = \underset{\mathbf{b}}{\operatorname{argmin}} || \mathbf{pupil}  - \mathbf{H} * \mathbf{b}||_{2}^{2}
\end{align}

This can be solved by means of the well-known normal equations. However, since negative spikes in demand are not plausible, according to Hoeks & Levelt, they subsequently also set all implausible spikes (i.e., negative ones) in $\hat{\mathbf{b}}$ to 0. This approach to obtaining a **non-negative least squares** (NNLS) spike weight estimate, from now on denoted as $\hat{\mathbf{b}}^*$, is sub-optimal (see Ang, 2020) and the authors acknowledged themselves that their work was limited to clearly separable and individual 'spikes in demand'. In 2012 Wierda and colleagues improved on this, by revealing that they can track more continuous-like changes in demand with 'spikes' assumed to happen every 100 ms. They relied on gradient-free constrained optimization to estimate non-negative spike weights and also introduced not just a scaling parameter to the original response function by Hoeks & Levelt, but also a slope parameter accounting for possible drifts in the pupil dilation time-course. Their algorithm iteratively improves and constrains $\hat{\mathbf{b}}^*$ to adhere to the physical limitations characterizing this estimation problem (i.e., only non-negative weights are permitted). Their work has inspired multiple replications and extensions (notably: Denison et al., 2020; see Fink et al. (2021) for a review).

Wierda et al. (2012) suggest that the temporal resolution of their approach is truly just limited by the selected sampling rate - promising that continuous changes in demand can be revealed. However, we noticed that the gradient free optimizer struggled with identifying solutions in case spikes were assumed to happen at every sample (even in their own experiments, they averaged over 100 runs of each 100.000 optimization steps to account for these fluctuations). This repository thus provides an implementation of an accelerated projected gradient descent optimizer (Ang, 2020; Sutskever et al., 2013), which makes progress on the optimization problem much faster and more reliably (see the other vignettes on convergence properties).

However, we observed that changing the optimizer was not sufficient to achieve the desired 20 ms spike spread: There is not enough bias constraining the NNLS estimate of $\hat{\mathbf{b}}^*$, which results in very flexible estimates that tend to over-estimate climax points in the demand trajectories and to show high variance! We show this below for some artificial data.

## Simulate Pupil Data

Here we simulate trial-level pupil data. When the *should_plot* argument is set to *True*, the function will generate five plots. The first plot shows how the simulated demand changes on the population level (i.e., the general trend that best describes changes in demand over time for all subjects belonging to a population of interest). The second plot shows the resulting pupil dilation time-course that is obtained by convolving the aforementioned demand trajectory with the pupil response function (Hoeks & Levelt, 1993). Plot three and four show similar information, but for individual subjects: the simulation assumes that there are differences (controlled via the *sub_dev* argument) between subjects with respect to how demanding a task is for them. The red lines thus reflect the individual subjects' true demand trajectories and corresponding pupil dilation time-courses. The fifth and final plot shows the final artificial raw data. Each line here represents a trial from an individual subject, obtained by adding to the subject's true demand level per-trial variation in the demand trajectories (i.e., trial-level variation in the spike weights, controlled via the *trial_dev* argument) and also (residual) normally distributed noise with constant standard deviation (controlled via the *residual_dev* argument). Finally, each trial can additionally feature a trend in the pupil dilation time-course (variance in slope terms is controlled via the *slope_dev* argument).

```{r}
n <- 5
sim_obj <- additive_pupil_sim(n_sub = n,
                              slope_dev = 1.5,
                              sub_dev = 0.15,
                              trial_dev = 0.25,
                              residual_dev=15.0,
                              should_plot=T)
dat <- sim_obj$data
```

## Aggregate

We now need to aggregate the trial-level data. The code below forms simple averages over time and subject. This is fine if all trials are subject to the same experimental manipulations but is not that informative if the experimental setup contains multiple manipulations (e.g., different (categorical) types of stimuli, or a manipulations of a continuous predictor variable). In that case, it is preferable to estimate an additive model of the pupil (for example using 'mgcv', see Wood (2017)) that takes into account the effects of these predictors on the size of the pupil. These 'smart aggregates' of the pupil can then be used to predict the pupil dilation time-course for specific constellations of the predictors taken into account by the model. The averages below can then be replaced with these pupil dilation time-course predictions.

```{r}
# ToDo: Use smart-aggregates here instead of averaging.
aggr_dat <- aggregate(list("pupil"=dat$pupil),by=list("subject"=dat$subject,"time"=dat$time),FUN=mean)

aggr_dat <- aggr_dat[order(aggr_dat$subject),]

plot(aggr_dat$time[aggr_dat$subject == 1],
     aggr_dat$pupil[aggr_dat$subject == 1],
     type="l",ylim=c(min(aggr_dat$pupil),
                     max(aggr_dat$pupil)),
     xlab="Time",
     ylab="Pupil dilation",
     main= "Average pupil trajectories for all subjects",
     lwd=3)

for (si in 2:n) {
 sub_aggr_dat <- aggr_dat[aggr_dat$subject == si,]
 lines(sub_aggr_dat$time,sub_aggr_dat$pupil,col=si,lwd=3)
}

```

## Defining possible pulse locations

In most cases it is advisable to just assume a possible demand spike at every pupil sample (i.e., every 20 ms in case the observed pupil dilatiopn time-course was down-sampled to 50 HZ). However, pulses located at the end of the time range considered (here > 2500) will not be associated with a complete pupil spline basis (i.e., the pupil response will be truncated). Already Wierda et al. (2012) reported the corresponding pulses to be commonly over-estimated and recommended that it is usually a good idea to drop the pulses corresponding to the last 3-5 response functions.

```{r}
# Calculate pulse locations
last_pulse_dat <- length(unique(aggr_dat$time)) - (3 * round(930/100)) - 1
  
# Define possible pulse location
pulse_locations_dat <- seq(1,last_pulse_dat,1)
real_locations_dat <- unique(aggr_dat$time)[pulse_locations_dat]
```

## Recover spike weights using papss

With the aggregates and a range of possible pulse locations at hand we can now attempt to recover the subjects' demand trajectories. We set the `start_lambda` argument to 0.0 and `maxiter_outer` to 1 to ensure that the optimization goal is really just the standard least squares objective with $\hat{\mathbf{b}}$ being constrained to contain only elements larger or equal to zero. `model` is set to `"WIER_SHARED"` to tell the optimizer to add slope terms for each subject's average pupil dilation time-course as introduced by Wierda et al. (2012).

```{r}
# Solve pupil spline
solvedPupil <- papss::pupil_solve(pulse_locations_dat,
                           real_locations_dat,
                           data = aggr_dat,
                           maxiter_inner = 1000000,
                           maxiter_outer = 1,
                           model="WIER_SHARED",
                           convergence_tol = 1e-06,
                           start_lambda = 0.0)

recovered_coef <- solvedPupil$coef
model_mat <- solvedPupil$modelmat
```

## Visualizing model setup

It is a good idea to also inspect the model matrix from this more complex model. The first n column contain slope terms. The remaining columns are easily visually separated into distinct blocks per subjects, showing the shifting pupil basis functions. These blocks of shifted basis functions can are also visualized in the second plot.

```{r}
heatmap(model_mat, Colv = NA, Rowv = NA, scale = "none")
# Plot bases against row-index (entire time range for all subjects)
plot(model_mat[,(n+1)],
     type="l",
     main="Basis setup over the entire time range")
for(ci in (n+2):ncol(model_mat)){
  lines(1:nrow(model_mat),model_mat[,ci],col=ci)
}
```

## Visualize estimates
Finally, we can take a look at the recovered demand trajectories and predicted pupil time-courses (plotted in red against the observed true ones). The estimates obtained here highlight the aforementioned problem surrounding the traditional NNLS estimate: the estimates show extreme peaks around climaxes in the true demand trajectory and generally high variance. Especially for individual subjects, the smooth ramp-ups/decreases are completely lost:

```{r}
par(mfrow=c(1,2))

# This helper function can be used to visualize true trajectories and the recovered estimates.
# Note: This only works if you specify model="WIER_SHARED" in the call to pupil_solve()!
plot_sim_vs_recovered(n,
                      aggr_dat,
                      sim_obj,
                      recovered_coef,
                      pulse_locations_dat,
                      real_locations_dat)

legend("topright",
       c("True trajectory","Unpenalized papss estimate"),
       col=c("black","red"),
       lwd=3,
       lty=1)
```


To improve on this, we augmented the optimization objective solved by the optimizer - to reflect estimation of a penalized additive model (Wood, 2017) of the pupil. For our toy-problem with four possible spikes, the optimization goal is now to find the set of weights that minimzes:


\begin{align}
\bar{\mathbf{b}^{*}} = \underset{\mathbf{b}}{\operatorname{argmin}} || \mathbf{pupil}  - \mathbf{H} * \mathbf{b}||_{2}^{2} + \lambda * \mathbf{b}^{T}*\mathbf{S}*\mathbf{b}
\end{align}

with $\bar{\mathbf{b}^{*}}$ again being subject to constraints enforcing it to contain only non-negative elements. $\mathbf{S}$ is called a penalty matrix (Wood, 2017).


In the current toy example, $\mathbf{S}$ would be a 4 by 4 matrix. The content of the matrix controls what aspects of the model are penalized specifically. For example, setting $\mathbf{S}$ to a 4by 4 identity matrix:

```{r}
S <- diag(nrow=4,ncol=4)

print(S)
```

ensures that the squared Euclidean norm of the current estimate $\bar{\mathbf{b}^{*}}$ weighed by $\lambda$ is added to the optimization problem:

```{r}
b_hat <- matrix(betas,ncol = 1)
lambda <- 0.1

# Calculating the penalty for a given lambda.
penalty <- lambda * t(b_hat) %*% S %*% b_hat

# The alternative way to calculate this, without matrix operations:
penalty_alt <- lambda * sum(betas**2)

# These should be equal:
print(all.equal(penalty[1,1],penalty_alt))
```

This recovers the [ridge regression optimization goal](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression), where coefficients are drawn towards zero (this is referred to as `shrinkage`). The framework of penalized additive models howevers permits for a broad range of different penalty matrices (even for combining multiple penalty matrices). The interested reader should consult Wood's introduction to generalized additive models (2017).

As briefly mentioned before the penalty matrix is parameterized by $\lambda$ (more specifically, the degree to which the penalty influences the optimization objective depends on the value of $\lambda$) - which the optimizer optionally estimates in an outer loop (max. number of iterations is controlled via `maxiter_outer`) for the current (i.e., given the current value for $\lambda$) best NNLS estimate $\bar{\mathbf{b}^{*}}$. More specifically, the generalized Fellner Schall update by Wood & Fasiolo is applied to $\lambda$ based on the current estimate for $\bar{\mathbf{b}^{*}}$ to maximize the restricted maximum likelihood of the **penalized additive model of the pupil size**.

Apart from reducing the variance in the estimator, adding the penalty also allows to make explicit the assumption that demand should change similarly for subjects. Since the same lambda value can be shared by multiple smooths' penalties (`mgcv` does this for the [factor-smooth basis](https://www.rdocumentation.org/packages/mgcv/versions/1.8-26/topics/smooth.construct.fs.smooth.spec), set via `bs=fs`.), a shared penalty can be enforced for all subjects. This enforces that the individual spike trajectories recovered for each subject, must be similarly smooth (Wood, 2017).

```{r}
#ToDo: Discuss  that penalty acts as 'improper gaussian prior' (Wood & Fasiolo, 2017) -> here NNLS also acts as uniform prior with support >= 0. This is also why we cannot calculate certainty CIs for our weights as done in 'mgcv', i.e., we violate that assumption.
```

Below we show, that an identity penalty with shared $\lambda$ term, placed on the coefficients of each subject's pupil spline, results in a much better estimate of the original demand trajectory:

```{r}
# Solve pupil spline
solvedPupil <- papss::pupil_solve(pulse_locations_dat,
                           real_locations_dat,
                           data = aggr_dat,
                           maxiter_inner = 100000,
                           maxiter_outer = 10, # 10-15 estimations are necessary.
                           model="WIER_SHARED",
                           convergence_tol = 1e-06,
                           start_lambda = 0.1) # Initial lambda MUST be > 0.0

recovered_coef <- solvedPupil$coef
model_mat <- solvedPupil$modelmat
```

```{r}
par(mfrow=c(1,2))

plot_sim_vs_recovered(n,
                      aggr_dat,
                      sim_obj,
                      recovered_coef,
                      pulse_locations_dat,
                      real_locations_dat)

legend("topright",
       c("True trajectory","Penalized papss estimate"),
       col=c("black","red"),
       lwd=3,
       lty=1)
```

## Final remarks

`papss` offers multiple model setups, not just one inspired by Wierda et al.'s setup (i.e., with extra slope terms). Dension et al. (2020) for example included intercept terms for each subject, and setting `model="DEN_SHARED"` will  similarly estimate intercept terms instead of slope terms, while setting `model=WIER_DEN_SHARED` will estimate both intercept and slope terms for each subject.

The demand on the population level was here calculated simply by averaging over all subject's individual recovered trajectories. If there is a lot of by-subject variation, then averaging will not be appropriate. Additive mixed effect models should be used instead (Wood, 2017). However, using the individual subjects' demand trajectories directly as dependent variable for an additive model might not be appropriate because demand is constrained to be non-negative. Fortunately in most experiments the research goal of interest would involve comparing demand levels between two or more conditions. In that case, demand difference curves can be calculated for each subject, which can then be used as dependent variable for an additive mixed model. The model should include a fixed smooth over time and factor-smooths over time for each subject (see Wood, 2017). The fixed effect will then easily reveal during which periods demand was significantly different between the experimental conditions on the population level.

## References

Ang, A. (2020a). Accelerated gradient descent for large-scale optimization: On Gradient descent solving Quadratic problems—Guest lecture of MARO 201—Advanced Optimization. https://angms.science/doc/teaching/GDLS.pdf

Ang, A. (2020b). Nonnegative Least Squares—PGD, accelerated PGD and with restarts. https://angms.science/doc/NMF/nnls_pgd.pdf

Denison, R. N., Parker, J. A., & Carrasco, M. (2020). Modeling pupil responses to rapid sequential events. Behavior Research Methods, 52(5), 1991–2007. https://doi.org/10.3758/s13428-020-01368-6

Fink, L., Simola, J., Tavano, A., Lange, E. B., Wallot, S., & Laeng, B. (2021). From pre-processing to advanced dynamic modeling of pupil data. PsyArXiv. https://doi.org/10.31234/osf.io/wqvue

Hoeks, B., & Levelt, W. (1993). Pupillary dilation as a measure of attention: A quantitative system analysis. Behav. Res. Meth. Ins. C., 25, 16–26.

Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013). On the importance of initialization and momentum in deep learning. Proceedings of the 30th International Conference on Machine Learning, 1139–1147. https://proceedings.mlr.press/v28/sutskever13.html

Wierda, S. M., van Rijn, H., Taatgen, N. A., & Martens, S. (2012). Pupil dilation deconvolution reveals the dynamics of attention at high temporal resolution. Proceedings of the National Academy of Sciences of the United States of America, 109(22), 8456–8460. https://doi.org/10.1073/pnas.1201858109

Wood, S. N., & Fasiolo, M. (2017). A generalized Fellner-Schall method for smoothing parameter optimization with application to Tweedie location, scale and shape models. Biometrics, 73(4), 1071–1081. https://doi.org/10.1111/biom.12666

Wood, S. N. (2017). Generalized Additive Models: An Introduction with R, Second Edition (2nd ed.). Chapman and Hall/CRC.

