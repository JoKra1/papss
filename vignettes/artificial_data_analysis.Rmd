---
title: "Artificial data analysis"
header-includes:
    - \usepackage{bm}
output:
  rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{artificial_data_analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.width = 6,
  fig.height = 4
)
```

```{r setup}
library(papss)
library(mgcv)
set.seed(2022) # For replicability.
```

## Introduction

Hoeks & Levelt (1993) investigated the characteristics of the function relating an increase in 'cognitive demand' to a change in the dilation of the pupil. They briefly summarize the extensive body of research relating changes in pupil dilation to the effort associated with cognitive processing - but Kahneman's book 'Attention and Effort' already published in 1973 might provide a better overview for the interested reader. Hoeks & Levelt were motivated by the fact that the pupil dilation time-course is best described as a slowly evolving signal, which obscures how changes in demand exactly contributed to its shape. They revealed that the increase in dilation following a spike in demand is best modeled using a Gamma-Erlang function. They also determined the optimal values for the parameters associated with the latter experimentally. You can check the documentation of the *h_basis* function if you want to know more (or read their paper!):

```{r, eval=F, echo=T}
help(h_basis)
```

In their original work they really focused on isolated 'demand spikes' linked to clearly identifiable events in their experiment setup. Below we visualize their 'pupil-response function':

```{r}
# First we create a time variable.
time <- seq(0,2500,by=20) # Time at 50 HZ

# Index vector for possible pulse locations, pointing to each sample
pulse_locations <- seq(1,length(time),by=1)

# Now we can calculate the effect on the pupil of an isolated spike
# that happened during the 5th. sample.
# Note that the 'f' parameter was added later by Wierda et al. (2012) to
# scale the response of the pupil to a spike of fixed strength.
pupil_dilation <- h_basis(5,time,0,time,pulse_locations,n=10.1,t_max=930,f=1/(10^24))

# Which we can now visualize...
plot(time,pupil_dilation,typ="l",lwd=3,xlab="time",ylab="Pupil dilation")

# Together with the spike...
abline(v=time[5],lty=2,lwd=3,col="red")

legend("topright",
       c("Pupil response","Spike in demand"),
       lty = c(1,2),lwd=3,col=c("black","red"))
```

This figure exemplifies what Hoeks & Levelt meant when they said that the pupil dilation changes slowly with time. It also highlights the contribution of the 't_max' parameter, since the pupil response peaks around 930 ms after the spike occurred.

According to their analysis, the **pupil dilation time-course** we end up observing then is nothing more than the accumulation of multiple **weighted** spikes (assuming that external conditions such as illumination and gaze direction remain stationary) - each having an additive effect on the dilation of the pupil:

```{r}
# Now lets add some more spikes - and vary their weight!

# This variable will hold the sum of the individual responses - i.e., what we could observe!
pupil <- rep(0,length.out=length(time))

# We now refer to the resulting variables as h_{i}, the purpose will become
# clear in the new paragraph.
H <- matrix(nrow=length(time),ncol = 4)
h1 <- h_basis(5,time,0,time,pulse_locations,n=10.1,t_max=930,f=1/(10^24))
H[,1] <- h1
# Don't forget to add the weighted contribution of this spike to the overall time-course
pupil <- pupil + 0.3 * h1

# Visualization of the individual contribution
plot(time,0.3 * h1,typ="l",
     lwd=3,xlab="time",
     ylab="Pupil dilation",
     ylim=c(0,70),lty=2)

# Together with the spike...
abline(v=time[5],lty=2,lwd=3,col="black")

# Sample some weights
betas <- runif(3)

# Now add more spikes!
iw <- 1
for (si in c(10,15,20)) {
  hi <- h_basis((si*3),time,0,time,pulse_locations,n=10.1,t_max=930,f=1/(10^24))
  pupil <- pupil + betas[iw] * hi
  H[,(iw+1)] <- hi
  
  lines(time,betas[iw] * hi,lwd=3,lty=2,col=si)
  abline(v=time[(si*3)],lty=2,lwd=3,col=si)
  iw <- iw +1
}

# And the resulting pupil dilation time-course we could actually observe!
lines(time,pupil,lwd=3)

# Merge weights for later
betas <- c(0.3,betas)
```

The weight of these spikes is of course only selected at random here for illustrative purposes. In practice, what researchers are interested in is to find the spike weights that most likely generated an observed pupil dilation time-course. Hoeks & Levelt recommended that this can for example be solved by means of optimizing a least squares problem - which in the end boils down to solving an **additive model of the pupil size**. For our toy example with 4 spikes, this would look like this:

\begin{align}
pupil_i = \beta_{1} * h_{1}(time_{i}) + \beta_{2} * h_{2}(time_{i}) + \beta_{3} * h_{3}(time_{i}) + \beta_{4} * h_{4}(time_{i}) + \epsilon_i
\end{align}

or more conveniently:

\begin{align}
pupil_{i} = \sum_{k=1}^{4} \beta_{k} * h_{k}(time_{i}) + \epsilon_i
\end{align}


A few remarks about the formulas above:

- The true weights (the $\beta_{i}$) associated with the individual pupil response functions are assumed to be non-negative since Hoeks & Levelt (1993) did not assume that a negative contribution to the observed pupil dilation time-course (resulting from a negative $\beta_{i}$) was plausible. Because of this **prior knowledge** we can impose constrains on the estimated $\hat{\beta_{i}}$ to be non-negative as well.

- In the simple additive model described above, the $\epsilon_i$ are assumed to be normally distributed around zero. This might not necessarily be true and reflects a common assumption to make estimation of parameters easier (see Slawski & Hein, 2014 for a more detailed discussion in the context of non-negative least squares optimization). In principle, the methods used in the package here could be extended to work without this explicit assumption by utilizing the methods described by Wood (2017) to estimate a generalized additive model of the pupil size.

If we now treat $\mathbf{b}$ as the vector containing the four individual $\beta_i$ and $\mathbf{H}$ is a matrix where each of the four columns corresponds to a $h_i$, with each row corresponding to an individual $h_i({time_{i}})$ (visualized below),

```{r}
# This is the matrix H, with 4 columns, each corresponding to one of the hi
# visualized earlier.
heatmap(H,Colv = NA,Rowv=NA,scale = "none")
```


then they argue that one option to recover the values for the $\beta_i$ is to solve the following least squares optimization problem:

\begin{align}
\hat{\mathbf{b}} = \underset{\mathbf{b}}{\operatorname{argmin}} || \mathbf{pupil}  - \mathbf{H} * \mathbf{b}||_{2}^{2}
\end{align}

Hoeks & Levelt mention that this can be solved by means of the well-known normal equations. However, since negative spikes in demand are not plausible, this approach to obtaining a **non-negative least squares** (NNLS) spike weight estimate, from now on denoted as $\hat{\mathbf{b}}^*$, is sub-optimal (see Ang, 2020) and the authors acknowledged themselves that their work was limited to clearly separable and individual 'spikes in demand'. In 2012 Wierda and colleagues improved on this, by revealing that they can track more continuous-like changes in demand with 'spikes' assumed to happen every 100 ms. They relied on gradient-free constrained optimization to estimate non-negative spike weights and also introduced not just a scaling parameter to the original response function by Hoeks & Levelt, but also a slope parameter accounting for possible drifts in the pupil dilation time-course. Their algorithm iteratively improves and constrains $\hat{\mathbf{b}}^*$ to adhere to the physical limitations characterizing this estimation problem (i.e., only non-negative weights are permitted). Their work has inspired multiple replications and extensions (notably: Denison et al., 2020; see Fink et al. (2021) for a review).

Wierda et al. (2012) suggest that the temporal resolution of their approach is truly just limited by the selected sampling rate - promising that continuous changes in demand can be revealed. However, we noticed that the gradient free optimizer struggled with identifying solutions in case spikes were assumed to happen at every sample (even in their own experiments, they averaged over 100 runs of 100.000 optimization steps each, to account for these fluctuations). This repository provides an implementation of an accelerated projected gradient descent optimizer (Ang, 2020; Sutskever et al., 2013), which makes progress on the optimization problem much faster and more reliably (see the other vignettes on convergence properties).

However, we observed that changing the optimizer alone was not sufficient to truely recover continuous changes in demand: The reported 'self-regularizing property of NNLS' (Slawski & Hein, 2014) - promotes sparse estimates that overly accentuate peaks in the demand trajectories. This results in demand trajectory estimates that for individual subjects cannot truly be described as continuous. Furthermore, it still appears that there is not enough bias constraining the NNLS estimate of $\hat{\mathbf{b}}^*$, resulting in quite flexible estimates that show high variance! We highlight these problems with some artificial data in the next section.

## Simulate Pupil Data

Here we simulate trial-level pupil data. When the *should_plot* argument is set to *True*, the function will generate five plots. The first plot shows how the simulated demand changes on the population level (i.e., the general trend that best describes changes in demand over time for all subjects belonging to a population of interest). The second plot shows the resulting pupil dilation time-course that is obtained by convolving the aforementioned demand trajectory with the pupil response function (Hoeks & Levelt, 1993). Plot three and four show similar information, but for individual subjects: the simulation assumes that there are differences (controlled via the *sub_dev* argument) between subjects with respect to how demanding a task is for them. The red lines thus reflect the individual subjects' true demand trajectories and corresponding pupil dilation time-courses. The fifth and final plot shows the final artificial raw data. Each line here represents a trial from an individual subject, obtained by adding to the subject's true demand level per-trial variation in the demand trajectories (i.e., trial-level variation in the spike weights, controlled via the *trial_dev* argument) and also (residual) normally distributed noise with constant standard deviation (controlled via the *residual_dev* argument). Finally, each trial can additionally feature a trend in the pupil dilation time-course (variance in slope terms is controlled via the *slope_dev* argument).

```{r}
n <- 5
sim_obj <- additive_pupil_sim(n_sub = n,
                              slope_dev = 1.5,
                              sub_dev = 0.15,
                              trial_dev = 0.25,
                              residual_dev=15.0,
                              should_plot=T)
dat <- sim_obj$data
```

## Aggregate

We now need to aggregate the trial-level data. The code below forms simple averages over time and subject. This is fine if all trials are subject to the same experimental manipulations but is not that informative if the experimental setup contains multiple manipulations (e.g., different (categorical) types of stimuli, or a manipulations of a continuous predictor variable). In that case, it is preferable to estimate an additive model of the pupil (for example using 'mgcv', see Wood (2017)) that takes into account the effects of these predictors on the size of the pupil. These 'smart aggregates' of the pupil can then be used to predict the pupil dilation time-course for specific constellations of the predictors taken into account by the model. The averages below can then be replaced with these pupil dilation time-course predictions.

```{r}
# ToDo: Use smart-aggregates here instead of averaging.
aggr_dat <- aggregate(list("pupil"=dat$pupil),by=list("subject"=dat$subject,"time"=dat$time),FUN=mean)

aggr_dat <- aggr_dat[order(aggr_dat$subject),]

plot(aggr_dat$time[aggr_dat$subject == 1],
     aggr_dat$pupil[aggr_dat$subject == 1],
     type="l",ylim=c(min(aggr_dat$pupil),
                     max(aggr_dat$pupil)),
     xlab="Time",
     ylab="Pupil dilation",
     main= "Average pupil trajectories for all subjects",
     lwd=3)

for (si in 2:n) {
 sub_aggr_dat <- aggr_dat[aggr_dat$subject == si,]
 lines(sub_aggr_dat$time,sub_aggr_dat$pupil,col=si,lwd=3)
}

```

## Defining possible pulse locations

Based on the findings by Wierda et al. (2012), that demand changes more like a continuous signal, the most sensible assumption regarding 'spike locations' is to assume a possible demand spike at every pupil sample (i.e., every 20 ms in case the observed pupil dilatiopn time-course was down-sampled to 50 HZ). Papss automatically calculated the possible locations, the spacing between spikes is controlled via the `pulse_spacing` argument. As argued before, this should usually be set to 1.

Additionally, an important question to consider is where to start assuming spikes in demand. In the simulation here, demand really starts to increase only within the time-window of interest. That is a reasonable assumption if we can establish a robust baseline period, so that after subtracting the baseline from our pupil signal the signal starts with an amplitude of zero and then only increases after stimulus onset, at which point demand can safely be assumed to start increasing. However, establishing such a robust baseline is often difficult with real data. It might be that already before stimulus onset, demand fluctuates. Because of the lagged pupil response, pulses outside of the time-window (here pulses before time-point 0) will still influence the pupil response within the time-window of interest. Papss by default tries to correct for this possibility, by modelling truncated pupil basis response functions that would originate from pulses within 500 ms before the time-window of interest (controlled via the `expand_by` argument). Pulses even further away are usually not that crucial to model, because even if these play a role, then the outsider pulses before 500 ms will simply be slightly over-estimated to compensate for this. It is also not possible to model even more pulses, because then the number of columns in the model matrix will quickly exceed the number of rows (data-points), which will render the problem unidentifiable. If it is absolutely crucial to estimate much earlier pulses, then the `pulse_dropping_factor` can be increased, which controls how many pulses are dropped at the end of the time-window. As discussed by Wierda et al., pulses far towards the end of the time-window are often over-estimated as well, and by default papss thus drops some of those. Increasing the amount of dropped pulses, obviously frees more basis functions that can be dedicated to account for baseline fluctuations, so in practice researchers will usually have to find a good balancing act here. Papss will prompt a warning and cancel estimation, in case the problem becomes unidentifiable.

Finally, the aforementioned scaling parameter `f` of the pupil response function, introduced by Wierda et al. is generally not known. A good estimate can often be based on the rounded median pupil size across the entire data set plus the standard deviation, scaled by a factor of 0.1 and 0.01 respectively (the sum is then further scaled). Recovering the precise value is also not crucial, since the shape of the pattern will still be recovered, up to a difference in a scaling factor.

With the aggregates and a range of possible pulse locations at hand we can now attempt to recover the subjects' demand trajectories. We set the `start_lambda` argument to 0.0 and `maxiter_outer` to 1 to ensure that the optimization goal is really just the standard non-negative least squares objective with $\hat{\mathbf{b}}$ being constrained to contain only elements larger or equal to zero. `model` is set to `"WIER_SHARED"` to tell the optimizer to add slope terms for each subject's average pupil dilation time-course as introduced by Wierda et al. (2012).

```{r}
# Solve pupil spline
solvedPupil <- papss::pupil_solve(pulse_spacing = 1,
                           data = aggr_dat,
                           maxiter_inner = 1000000,
                           maxiter_outer = 1,
                           model="WIER_SHARED",
                           start_lambda = 0.0,
                           expand_by = 500,
                           pulse_dropping_factor = 3,
                           f=(1/(10^(round(median(aggr_dat$pupil)/10) + round(sd(aggr_dat$pupil)/100)))))

recovered_coef <- solvedPupil$coef
model_mat <- solvedPupil$modelmat
```

## Visualizing model setup

It is a good idea to also inspect the model matrix from this more complex model. The first n column contain slope terms. The remaining columns are easily visually separated into distinct blocks per subjects, showing the shifting pupil basis functions. These blocks of shifted basis functions can are also visualized in the second plot.

```{r}
heatmap(model_mat, Colv = NA, Rowv = NA, scale = "none")
# Plot bases against row-index (entire time range for all subjects)
plot(model_mat[,(n+1)],
     type="l",
     main="Basis setup over the entire time range")
for(ci in (n+2):ncol(model_mat)){
  lines(1:nrow(model_mat),model_mat[,ci],col=ci)
}
```

## Visualize estimates
Finally, we can take a look at the recovered demand trajectories and predicted pupil time-courses (plotted in red against the observed true ones). The estimates obtained here highlight the aforementioned problem surrounding the traditional NNLS estimate: for individual subjects the estimates show extreme peaks around climaxes in the true demand trajectory. The smooth ramp-ups/decreases are completely lost. The population level estimate (a simple average over subjects) reveals that this estimate permits for very flexible changes in demand and does not represent the smooth changes in demand very well.

```{r}
par(mfrow=c(1,2))

# This helper function can be used to visualize true trajectories and the recovered estimates.
# Note: This only works if you specify model="WIER_SHARED" in the call to pupil_solve()!
plot_sim_vs_recovered(n,
                      aggr_dat,
                      sim_obj,
                      recovered_coef,
                      solvedPupil$pulseLocations,
                      solvedPupil$realLocations,
                      solvedPupil$pulsesInTime,
                      solvedPupil$expandedTime,
                      (500/20),
                      (1/(10^(round(median(aggr_dat$pupil)/10) + round(sd(aggr_dat$pupil)/100)))),
                      1) # Scaling factor that potentially got lost through `f` estimate - here no loss.

legend("topright",
       c("True trajectory","Unpenalized papss estimate"),
       col=c("black","red"),
       lwd=3,
       lty=1)
```


To improve on this, we augmented the optimization objective solved by the optimizer - to reflect estimation of a penalized additive model (Wood, 2017) of the pupil. For our toy-problem with four possible spikes, the optimization goal is now to find the set of weights that minimzes:


\begin{align}
\bar{\mathbf{b}^{*}} = \underset{\mathbf{b}}{\operatorname{argmin}} || \mathbf{pupil}  - \mathbf{H} * \mathbf{b}||_{2}^{2} + \lambda * \mathbf{b}^{T}*\mathbf{S}*\mathbf{b}
\end{align}

with $\bar{\mathbf{b}^{*}}$ again being subject to constraints enforcing it to contain only non-negative elements. $\mathbf{S}$ is called a penalty matrix (Wood, 2017).


In the current toy example, $\mathbf{S}$ would be a 4 by 4 matrix. The content of the matrix controls what aspects of the model are penalized specifically. For example, setting $\mathbf{S}$ to a 4by 4 identity matrix:

```{r}
S <- diag(nrow=4,ncol=4)

print(S)
```

ensures that the squared Euclidean norm of the current estimate $\bar{\mathbf{b}^{*}}$ weighed by $\lambda$ is added to the optimization problem:

```{r}
b_hat <- matrix(betas,ncol = 1)
lambda <- 0.1

# Calculating the penalty for a given lambda.
penalty <- lambda * t(b_hat) %*% S %*% b_hat

# The alternative way to calculate this, without matrix operations:
penalty_alt <- lambda * sum(betas**2)

# These should be equal:
print(all.equal(penalty[1,1],penalty_alt))
```

This recovers the [ridge regression optimization goal](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression), where coefficients are drawn towards zero (this is referred to as `shrinkage`). The framework of penalized additive models howevers permits for a broad range of different penalty matrices (even for combining multiple penalty matrices). The interested reader should consult Wood's introduction to generalized additive models (2017). Using the identity penalty, i.e., drawing coefficients towards zero, was considered desirable because the NNLS estimates showed overly accentuated peaks - reflecting high magnitude values for some of the individual estimates in $\bar{\mathbf{b}^{*}}$.

As briefly mentioned before the penalty matrix is parameterized by $\lambda$ (more specifically, the degree to which the penalty influences the optimization objective depends on the value of $\lambda$) - which in GAMMS is commonly optimized in an outer loop (max. number of iterations is controlled via `maxiter_outer`) for the current (i.e., given the current value for $\lambda$) best least squares estimate $\bar{\mathbf{b}}$  (see Wood, 2017 for details). Note that the optimizers for $\lambda$ discussed by Wood (2017) were designed to work with unconstrained least square estimates. However, in practice, applying at least one of those optimizers to the current best NNLS estimate $\bar{\mathbf{b}}^*$ turns out to work quite well. More specifically, `papss` applies the generalized Fellner Schall update by Wood & Fasiolo to $\lambda$ based on the current estimate for $\bar{\mathbf{b}^{*}}$ to maximize the restricted maximum likelihood of the **penalized additive model of the pupil size**. As pointed out by the authors, the latter is only really dependent on the latest estimate of $\bar{\mathbf{b}}$, which we simply replace with $\bar{\mathbf{b}^{*}}$. While this is does not align with the original formulation of the update term, it should be considered that the NNLS estimate itself is part of the set of all weights to which the update could possibly be applied (see Bolduc et al., 2017) and that Wood (2017) shows that $\lambda$ updates can even be applied to intermediary estimates and still result in satisfactory estimates. 

Apart from reducing the variance in the estimator, adding the penalty also allows to make explicit the assumption that demand should change similarly for subjects. Since the same lambda value can be shared by multiple smooths' penalties (`mgcv` does this for the [factor-smooth basis](https://www.rdocumentation.org/packages/mgcv/versions/1.8-26/topics/smooth.construct.fs.smooth.spec), set via `bs=fs`.), a shared penalty can be enforced for all subjects. This enforces that the individual spike trajectories recovered for each subject, must be similarly smooth (Wood, 2017).

```{r}
#ToDo: Discuss  that penalty acts as 'improper gaussian prior' (Wood & Fasiolo, 2017) -> here NNLS also acts as uniform prior with support >= 0. This is also why we cannot calculate certainty CIs for our weights as done in 'mgcv', i.e., we violate that assumption.
```

Below we show, that applying the aforementioned update rule results in a much better estimate of the original demand trajectory:

```{r}
# Solve pupil spline
solvedPupil <- papss::pupil_solve(pulse_spacing = 1,
                           data = aggr_dat,
                           maxiter_inner = 100000,
                           maxiter_outer = 10,
                           model="WIER_SHARED",
                           start_lambda = 0.1,
                           expand_by = 500,
                           pulse_dropping_factor = 3,
                           f=(1/(10^(round(median(aggr_dat$pupil)/10) + round(sd(aggr_dat$pupil)/100)))))

recovered_coef <- solvedPupil$coef
model_mat <- solvedPupil$modelmat
```

```{r}
par(mfrow=c(1,2))

plot_sim_vs_recovered(n,
                      aggr_dat,
                      sim_obj,
                      recovered_coef,
                      solvedPupil$pulseLocations,
                      solvedPupil$realLocations,
                      solvedPupil$pulsesInTime,
                      solvedPupil$expandedTime,
                      (500/20),
                      (1/(10^(round(median(aggr_dat$pupil)/10) + round(sd(aggr_dat$pupil)/100)))),
                      1)

legend("topright",
       c("True trajectory","Penalized papss estimate"),
       col=c("black","red"),
       lwd=3,
       lty=1)
```

## Final remarks

`papss` offers multiple model setups, not just one inspired by Wierda et al.'s setup (i.e., with extra slope terms). Dension et al. (2020) for example included intercept terms for each subject, and setting `model="DEN_SHARED"` will  similarly estimate intercept terms instead of slope terms, while setting `model=WIER_DEN_SHARED` will estimate both intercept and slope terms for each subject.

The demand on the population level was here calculated simply by averaging over all subject's individual recovered trajectories. If there is a lot of by-subject variation, then averaging will not be appropriate. Additive mixed effect models should be used instead (Wood, 2017). However, using the individual subjects' demand trajectories directly as dependent variable for an additive model might not be appropriate because demand is constrained to be non-negative. Fortunately in most experiments the research goal of interest would involve comparing demand levels between two or more conditions. In that case, demand difference curves can be calculated for each subject, which can then be used as dependent variable for an additive mixed model. The model should include a fixed smooth over time and factor-smooths over time for each subject (see Wood, 2017). The fixed effect will then easily reveal during which periods demand was significantly different between the experimental conditions on the population level.

## References

Ang, A. (2020a). Accelerated gradient descent for large-scale optimization: On Gradient descent solving Quadratic problems—Guest lecture of MARO 201—Advanced Optimization. https://angms.science/doc/teaching/GDLS.pdf

Ang, A. (2020b). Nonnegative Least Squares—PGD, accelerated PGD and with restarts. https://angms.science/doc/NMF/nnls_pgd.pdf

Denison, R. N., Parker, J. A., & Carrasco, M. (2020). Modeling pupil responses to rapid sequential events. Behavior Research Methods, 52(5), 1991–2007. https://doi.org/10.3758/s13428-020-01368-6

Fink, L., Simola, J., Tavano, A., Lange, E. B., Wallot, S., & Laeng, B. (2021). From pre-processing to advanced dynamic modeling of pupil data. PsyArXiv. https://doi.org/10.31234/osf.io/wqvue

Hoeks, B., & Levelt, W. (1993). Pupillary dilation as a measure of attention: A quantitative system analysis. Behav. Res. Meth. Ins. C., 25, 16–26.

Slawski, M., & Hein, M. (2014). Non-negative least squares for high-dimensional linear models: Consistency and sparse recovery without regularization. ArXiv:1205.0953 [Math, Stat]. http://arxiv.org/abs/1205.0953

Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013). On the importance of initialization and momentum in deep learning. Proceedings of the 30th International Conference on Machine Learning, 1139–1147. https://proceedings.mlr.press/v28/sutskever13.html

Wierda, S. M., van Rijn, H., Taatgen, N. A., & Martens, S. (2012). Pupil dilation deconvolution reveals the dynamics of attention at high temporal resolution. Proceedings of the National Academy of Sciences of the United States of America, 109(22), 8456–8460. https://doi.org/10.1073/pnas.1201858109

Wood, S. N., & Fasiolo, M. (2017). A generalized Fellner-Schall method for smoothing parameter optimization with application to Tweedie location, scale and shape models. Biometrics, 73(4), 1071–1081. https://doi.org/10.1111/biom.12666

Wood, S. N. (2017). Generalized Additive Models: An Introduction with R, Second Edition (2nd ed.). Chapman and Hall/CRC.
