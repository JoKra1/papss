---
title: "Artificial data analysis"
header-includes:
    - \usepackage{bm}
output:
  rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{artificial_data_analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.width = 6,
  fig.height = 4
)
```

```{r setup}
library(papss)
library(mgcv)
set.seed(2022) # For replicability.
```

# First steps with ``papss``

## Introduction

Hoeks & Levelt (1993) investigated the characteristics of the function relating an increase in 'cognitive demand' to a change in the dilation of the pupil. They revealed that this function is best modeled using a Gamma-Erlang function. They also determined optimal general values for the parameters associated with the latter experimentally. You can check the documentation of the *h_basis* function if you want to know more (or read their paper!):

```{r, eval=F, echo=T}
help(h_basis)
```

In their original work they really focused on isolated 'demand spikes' linked to clearly identifiable events in their experiment setup. Below we visualize their 'pupil-response function':

```{r}
# First we create a time variable.
time <- seq(0,2500,by=20) # Time at 50 HZ

# Index vector for possible pulse locations, pointing to each sample
pulse_locations <- seq(1,length(time),by=1)

# Now we can calculate the effect on the pupil of an isolated spike
# that happened during the 5th. sample using the default parameters
# estimated by Hoeks & Levelt.
# Note that the 'f' parameter was added later by Wierda et al. (2012) to
# scale the response of the pupil to a spike of fixed strength.
pupil_dilation <- h_basis(5,time,0,time,pulse_locations,n=10.1,t_max=930,f=1/(10^24))

# Which we can now visualize...
plot(time,pupil_dilation,typ="l",lwd=3,xlab="time",ylab="Pupil dilation")

# Together with the spike...
abline(v=time[5],lty=2,lwd=3,col="red")

legend("topright",
       c("Pupil response","Spike in demand"),
       lty = c(1,2),lwd=3,col=c("black","red"))
```

This figure exemplifies what Hoeks & Levelt meant when they said that the pupil dilation changes slowly with time. It also highlights the contribution of the 't_max' parameter, since the pupil response peaks around 930 ms after the spike occurred.

According to their analysis, the observed  **pupil dilation time-course** is nothing more than the result of multiple **weighted** spikes - each having an additive effect on the dilation of the pupil:

```{r}
# Now lets add some more spikes - and vary their weight!

# This variable will hold the sum of the individual responses - i.e., what we could observe!
pupil <- rep(0,length.out=length(time))

# We now refer to the resulting variables as h_{i}, the purpose will become
# clear in the new paragraph.
H <- matrix(nrow=length(time),ncol = 4)
h1 <- h_basis(5,time,0,time,pulse_locations,n=10.1,t_max=930,f=1/(10^24))
H[,1] <- h1
# Don't forget to add the weighted contribution of this spike to the overall time-course
pupil <- pupil + 0.3 * h1

# Visualization of the individual contribution
plot(time,0.3 * h1,typ="l",
     lwd=3,xlab="time",
     ylab="Pupil dilation",
     ylim=c(0,70),lty=2)

# Together with the spike...
abline(v=time[5],lty=2,lwd=3,col="black")

# Sample some weights
betas <- runif(3)

# Now add more spikes!
iw <- 1
for (si in c(10,15,20)) {
  hi <- h_basis((si*3),time,0,time,pulse_locations,n=10.1,t_max=930,f=1/(10^24))
  pupil <- pupil + betas[iw] * hi
  H[,(iw+1)] <- hi
  
  lines(time,betas[iw] * hi,lwd=3,lty=2,col=si)
  abline(v=time[(si*3)],lty=2,lwd=3,col=si)
  iw <- iw +1
}

# And the resulting pupil dilation time-course we could actually observe!
lines(time,pupil,lwd=3)

# Merge weights for later
betas <- c(0.3,betas)
```

``papss`` tries to find the set of weights that most likely generated an observed pupil dilation time-course! However, ``papss`` is not limited to the assumption made by Hoeks & Levelt that there are only a couple of isolated spikes. Rather, it attempts to recover **continuous changes in the underlying demand trajectory**. The idea that demand changes continuously traces back to the work by Wierda and colleagues who revealed that they can track more continuous-like changes in demand with "spikes" assumed to happen every 100 ms. ``papss`` goes one step further and models truly continuous changes in demand, with "spikes" at every measured sample. This document provides instructions on how ``papss`` can be used for an analysis of the pupil dilation time-course.

## Simulate Pupil Data

Of course we first need data. Here we simulate trial-level data. When the *should_plot* argument is set to *True*, the function will generate five plots. The first plot shows how the simulated demand changes on the population level (i.e., the general trend that best describes changes in demand over time for all subjects belonging to a population of interest). The second plot shows the resulting pupil dilation time-course that is obtained by convolving the aforementioned demand trajectory with the pupil response function (Hoeks & Levelt, 1993). Plot three and four show similar information, but for individual subjects: the simulation assumes that there are differences (controlled via the *sub_dev* argument) between subjects with respect to how demanding a task is for them. The red lines thus reflect the individual subjects' true demand trajectories and corresponding pupil dilation time-courses. The fifth and final plot shows the final artificial raw data. Each line here represents a trial from an individual subject, obtained by adding to the subject's true demand level per-trial variation in the demand trajectories (i.e., trial-level variation in the spike weights, controlled via the *trial_dev* argument) and also (residual) normally distributed noise with constant standard deviation (controlled via the *residual_dev* argument). Finally, each trial can additionally feature a trend in the pupil dilation time-course (variance in slope terms is controlled via the *slope_dev* argument).

```{r}
n <- 5
sim_obj <- additive_pupil_sim(n_sub = n,
                              slope_dev = 1.5,
                              sub_dev = 0.15,
                              trial_dev = 0.25,
                              residual_dev=15.0,
                              should_plot=T)
dat <- sim_obj$data
```

## Aggregate

The method by Hoeks & Levelt, extensions (e.g., Wierda et al., 2020), and also ``papss`` require aggregated data. The code below forms simple averages over time and subject. This is fine if all trials are subject to the same experimental manipulations but is not that informative if the experimental setup contains multiple manipulations (e.g., different (categorical) types of stimuli, or a manipulations of a continuous predictor variable). In that case, it might be more desirable to estimate an additive model of the pupil (for example using 'mgcv', see Wood (2017)) that takes into account the effects of these predictors on the size of the pupil.

```{r}
aggr_dat <- aggregate(list("pupil"=dat$pupil),by=list("subject"=dat$subject,"time"=dat$time),FUN=mean)

aggr_dat <- aggr_dat[order(aggr_dat$subject),]

plot(aggr_dat$time[aggr_dat$subject == 1],
     aggr_dat$pupil[aggr_dat$subject == 1],
     type="l",ylim=c(min(aggr_dat$pupil),
                     max(aggr_dat$pupil)),
     xlab="Time",
     ylab="Pupil dilation",
     main= "Average pupil trajectories for all subjects",
     lwd=3)

for (si in 2:n) {
 sub_aggr_dat <- aggr_dat[aggr_dat$subject == si,]
 lines(sub_aggr_dat$time,sub_aggr_dat$pupil,col=si,lwd=3)
}

```

## Defining possible pulse locations

Based on the findings by Wierda et al. (2012), that demand changes more like a continuous signal, the most sensible assumption regarding 'spike locations' is to assume a possible demand spike at every pupil sample (i.e., every 20 ms in case the observed pupil dilatiopn time-course was down-sampled to 50 HZ). ``papss`` automatically calculates the possible locations, the spacing between spikes is controlled via the `pulse_spacing` argument. As argued before, this should usually be set to 1 (i.e., a "spike" every sample, a 2 would mean a "spike" every two samples. If the pupil dilation time-course is down-sampled to 50 HZ, then setting this to 5 leads to the spacing used by Wierda et al. in 2012).

``papss`` also attempts to correct for pulses that happened before the recording window! In the simulation here, demand really starts to increase only within the time-window of interest. However, this is often difficult with real data. It might be that already before stimulus onset, demand fluctuates. Because of the lagged pupil response, pulses outside of the time-window (here pulses before time-point 0) will then still influence the pupil response within the time-window of interest. ``papss`` by default tries to correct for this possibility, by modelling truncated pupil basis response functions that would originate from pulses within 500 ms before the time-window of interest (controlled via the `expand_by` argument). As discussed by Wierda et al., pulses far towards the end of the time-window are often over-estimated as well, and by default ``papss`` thus drops some of those (controlled via `pulse_dropping_factor`). Increasing the amount of dropped pulses, obviously frees more basis functions that can be dedicated to account for baseline fluctuations, so in practice researchers will usually have to find a good balancing act here. ``papss`` will prompt a warning and cancel estimation, in case the problem becomes unidentifiable.

Finally, the aforementioned scaling parameter `f` of the pupil response function, introduced by Wierda et al. is generally not known. The choice for this parameter depends largely on the selected pre-processing pipeline and the eye-tracker used for recording. For example, Wierda et al. (2012) used normalized pupil dilation time-courses. In that case a value of (1/(10^(27))) worked quite well. The simulation here generates data that is more in line with data that was only base-lined by means of subtraction. We achieved good results with a value of (1/(10^(24))) in that case. Recovering the precise value is also not crucial, since the shape of the pattern will still be recovered, up to a difference in a scaling factor!

## Estimating demand

With the aggregates we can now attempt to recover the subjects' demand trajectories. We set the `maxiter_outer` argument to to 10 to speed up estimation for this example. In practice, good convergence requires between 15-30 iterations of the outer optimizer. The inner optimizer usually takes 100000 (the default value) steps to get close to the optimal (in least squares sense) solution. `model` is set to `"WIER_SHARED"` to tell the optimizer to add slope terms for each subject's average pupil dilation time-course as introduced by Wierda et al. (2012).

```{r}
# Solve pupil spline
solvedPupil <- papss::pupil_solve(pulse_spacing = 1,
                           data = aggr_dat,
                           maxiter_inner = 100000,
                           maxiter_outer = 10,
                           model="WIER_SHARED",
                           expand_by = 500,
                           pulse_dropping_factor = 3,
                           f=(1/(10^(24))))

recovered_coef <- solvedPupil$coef
model_mat <- solvedPupil$modelmat
```

## Visualize estimates

Finally, we can take a look at the recovered demand trajectories and predicted pupil time-courses (plotted in red against the observed true ones):

```{r}
par(mfrow=c(1,2))
# This helper function can be used to visualize true trajectories and the recovered estimates.
# Note: This only works if you specify model="WIER_SHARED" in the call to pupil_solve()!
# It is also a helpful starting point if you want to make your own visualizations of the
# coefficients.

plot_sim_vs_recovered(n,
                      aggr_dat,
                      sim_obj,
                      recovered_coef,
                      solvedPupil$pulseLocations,
                      solvedPupil$realLocations,
                      solvedPupil$pulsesInTime,
                      solvedPupil$expandedTime,
                      (500/20),
                      (1/(10^(24))),
                      1)

legend("topright",
       c("True trajectory","Penalized papss estimate"),
       col=c("black","red"),
       lwd=3,
       lty=1)
```

# Advaced topics

## Parameter confidence

To get an estimate of parameter confidence we can rely on bootstrapping
(Efron & Tibshirani, 1993). ``papss`` supports residual replacing only. As discussed by
Efron & Tibshirani, this requires evaluating the residuals generated for our data
carefully:

```{r}
plot(solvedPupil$fitted,
     solvedPupil$resid,
     xlab="Fitted values",
     ylab = "Residuals")
abline(h=0,lty=2)

acf(solvedPupil$resid,
    main="Auto-correlation in residuals")
```

The residuals should show approximately constant variance and low correlation
across different lags (Efron & Tibshirani, 1993). Only then are the confidence
estimates, recovered below, to be trusted.

```{r}
# Perform bootstrap estimation.
bootstrap <- papss::bootstrap_papss_standard_error(solvedPupil$coef,
                                                   aggr_dat$pupil,
                                                   solvedPupil$setup,
                                                   N=25, # This should be higher in practice!
                                                   maxiter_inner = 100000,
                                                   maxiter_outer = 10,
                                                   f=(1/(10^(24))))
```

We can now visualize the individual demand trajectories with an estimate of the
uncertainty surrounding them.

```{r}
plot_sim_vs_recovered(n,
                      aggr_dat,
                      sim_obj,
                      recovered_coef,
                      solvedPupil$pulseLocations,
                      solvedPupil$realLocations,
                      solvedPupil$pulsesInTime,
                      solvedPupil$expandedTime,
                      (500/20),
                      (1/(10^(24))),
                      1,
                      se=bootstrap$standardError,
                      plot_avg=F)

legend("topright",
       c("True trajectory","Penalized papss estimate"),
       col=c("black","red"),
       lwd=3,
       lty=1)
```

## Tuning the t_max parameter

So far we simply used the parameters for the pupil response function provided by
Hoeks & Levelt (1996): ``t_max=930`` and  ``n=10.1``. In practice, these values are
good starting points but they are unlikely to be perfect. Denison et al., (2020) for
example reported that the former varied a lot between different subjects, but was relatively
stable across different tasks. They thus recommend that ``t_max`` should generally be estimated
as well. ``papss`` includes a cross-validation function based on the procedure recommended by
the authors to achieve this.

In this case, the true ``t_max`` used in the simulation is
recovered by this data-driven approach as well. Note that in practice, the
procedure should not be applied to data from multiple subjects. This worked
here because the simulation assumed a single shared ``t_max``. In practice, it would be more
appropriate to determine the parameter for each subject, using the data from all experimental
conditions as factor in the ``papss`` model. This is explained in more detail in the next section.

```{r}
# Method expects numeric trial variable for easy splitting.
dat$num_trial <- as.numeric(dat$trial)
  
# Randomize trial order
unq_trials <- unique(dat$num_trial)
unq_trials <- sample(unq_trials,length(unq_trials))

# Split data into n=4 folds
n_trials <- length(unq_trials)
fold_order <- cut(1:n_trials,4,labels = F)
folds <- list()
for(f in fold_order){
  folds[[f]] <- unq_trials[fold_order == f]
}

# Perform cross-validation to find tmax!
cross_val_errs <- papss::cross_val_tmax(cand_tmax=c(1130,1030,930,830,730),
                                        folds=folds,
                                        pulse_spacing = 1,
                                        trial_data = dat,
                                        maxiter_inner = 100000,
                                        maxiter_outer = 10,
                                        model="WIER_SHARED",
                                        start_lambda = 0.1,
                                        expand_by = 0,
                                        pulse_dropping_factor = 4,
                                        f=(1/(10^(24))))


```

## Experimental designs with multiple conditions

The simulation here was very limited in that it basically assumed a single condition
and that all subjects were very similar in how they experienced demand during the
hypothetical task performed during the condition. This is not reflective of real data.

In practice there will be data from multiple experimental conditions and subjects'
pupil dilation time-courses will differ a lot (Denison et al., 2020). In that case it
does not become desirable to fit a shared model for all subjects.

Rather, it would be more appropriate to determine for each subject the best ``t_max`` parameter. This can be done using data from all experimental conditions (see Denison et al. 2020). Then the best ``t_max`` model
for each subject should be fitted using the data from all experimental conditions. Note, this
does not mean that the same pupil dilation time-course will be assumed across all conditions.
``papss`` will still estimate a different demand trajectory **per condition**. To achieve this,
a column will have to exist in the data frame (needs to be a factor) that identifies the different factor
levels. The name of this column needs then be passed to the ``pupil_solve()`` and ``cross_val_tmax()`` functions
via the ``factor_id`` argument (default is "subject").

From this final model, difference in demand curves can then be calculated for each subject. E.g., the
difference between condition A and condition B as well as the difference between condition C and condition B. These can then be used as dependent variable for an additive mixed model to test whether there is a shared trend across subjects. The model should include a fixed smooth over time and factor-smooths over time for each subject (see Wood, 2017). The fixed effect will then easily reveal during which periods demand was significantly different between the experimental conditions on the population level.

This setup is outlined in the code example below:

```{r}
# First build condition dat
condDat <- NULL
n <- 15
for (ci in 1:2) {
  cDat <- additive_pupil_sim(n_sub = n,
                              slope_dev = 1.5,
                              sub_dev = 0.15,
                              trial_dev = 0.25,
                              residual_dev=15.0,
                              should_plot=T,
                              seed = 2022+ci)$data
  cDat$condition <- ci
  condDat <- rbind(condDat,cDat)
}

condDat$condition <- as.factor(condDat$condition)


# Now find the best t_max parameters for all subjects
best_t_max_all <- c()
tmax_candidates <- c(1130,1030,930,830,730)

for (si in 1:n) {
  # Get subject data
  sub_dat <- condDat[condDat$subject == si,]
  
  
  # Perform cross-validation for this subject!
  
  # Start with making continuous trial variable again
  sub_dat$num_trial <- as.numeric(sub_dat$trial)
  
  # Randomize trial order
  unq_trials <- unique(sub_dat$num_trial)
  unq_trials <- sample(unq_trials,length(unq_trials))
  
  # Split data into n=4 folds
  n_trials <- length(unq_trials)
  fold_order <- cut(1:n_trials,4,labels = F)
  folds <- list()
  for(f in fold_order){
    folds[[f]] <- unq_trials[fold_order == f]
  }
  
  # Perform cross-validation to find tmax!
  
  cross_val_errs <- papss::cross_val_tmax(cand_tmax=tmax_candidates,
                                          folds=folds,
                                          pulse_spacing = 1,
                                          trial_data = sub_dat,
                                          factor_id = "condition", # Estimate demand per condition for this subj.!
                                          maxiter_inner = 100000,
                                          maxiter_outer = 10,
                                          model="WIER_SHARED",
                                          start_lambda = 0.1,
                                          expand_by = 0,
                                          pulse_dropping_factor = 4,
                                          f=(1/(10^(24))))
  
  # Get best t_max for this subject!
  min_err <- min(cross_val_errs)
  best_tmax <- tmax_candidates[cross_val_errs == min_err]
  cat("Best tmax: ",best_tmax,"\n")
  best_t_max_all <- c(best_t_max_all,best_tmax)
}

# Now fit the best model for each subject

# Store the difference curves for analysis with mgcv
demand_diff_dat <- NULL
for (si in 1:n) {
  # Get subject data again
  sub_dat <- condDat[condDat$subject == si,]
  
  # Get subjects t_max
  subj_t_max <- best_t_max_all[si]
  
  # Now create by condition averages!
  aggr_cond_dat_subj <- aggregate(list("pupil"=sub_dat$pupil),by=list("time"=sub_dat$time,
                                                                      "condition"=sub_dat$condition),FUN=mean)
  
  aggr_cond_dat_subj <- aggr_cond_dat_subj[order(aggr_cond_dat_subj$condition),]
  
  # Solve the model for this subject
  solvedPupil <- papss::pupil_solve(pulse_spacing = 1,
                           data = aggr_cond_dat_subj,
                           factor_id = "condition", # Again, estimate demand per condition.
                           maxiter_inner = 100000,
                           maxiter_outer = 10,
                           model="WIER_SHARED",
                           start_lambda = 0.1,
                           expand_by = 500,
                           pulse_dropping_factor = 4,
                           f=(1/(10^(24))),
                           t_max = subj_t_max)
  
  # Get the demand trajectories for all conditions per subject
  demand_dat_sub <- papss::extract_demand_for_fact(aggr_cond_dat_subj,
                                                   solvedPupil$coef,
                                                   solvedPupil$pulseLocations,
                                                   solvedPupil$realLocations,
                                                   solvedPupil$pulsesInTime,
                                                   solvedPupil$expandedTime,
                                                   (500/20),
                                                   factor_id = "condition")
  
  # Calculate demand difference curve between two conditions per subject
  demad_diff <- demand_dat_sub$demand[demand_dat_sub$factor == 1] - 
                demand_dat_sub$demand[demand_dat_sub$factor == 2]
  
  demand_diff_dat_sub <- data.frame("demand_diff"=demad_diff,
                                    "time"=unique(demand_dat_sub$time),
                                    "subject"=si)
  
  demand_diff_dat <- rbind(demand_diff_dat,demand_diff_dat_sub)
  
}

# Now analyzie via mgcv:
demand_diff_dat$subject <- as.factor(demand_diff_dat$subject)

par(mfrow=c(2,2))
for(si in 1:n){
  plot(demand_diff_dat$time[demand_diff_dat$subject == si],
       demand_diff_dat$demand_diff[demand_diff_dat$subject == si],
       type="l",
       xlab="Time",
       ylab="Demand difference")
}

# Everything important happens before 2000 ms
demand_diff_dat <- demand_diff_dat[demand_diff_dat$time < 2000,]

par(mfrow=c(1,1))
m1 <- gam(demand_diff ~ s(time,k=15) + s(subject,bs="re"), data = demand_diff_dat)
plot(m1)
summary(m1)
gam.check(m1)
```

# Final remarks

`papss` offers multiple model setups, not just one inspired by Wierda et al.'s setup (i.e., with extra slope terms). Dension et al. (2020) for example included intercept terms for each subject, and setting `model="DEN_SHARED"` will  similarly estimate intercept terms instead of slope terms, while setting `model=WIER_DEN_SHARED` will estimate both intercept and slope terms for each subject. See:

```{r, eval=F, echo=T}
help(WIER_SHARED_NNLS_model_setup)
help(WIER_IND_NNLS_model_setup)
help(DEN_SHARED_NNLS_model_setup)
help(WIER_DEN_SHARED_NNLS_model_setup)
```


# References

Ang, A. (2020a). Accelerated gradient descent for large-scale optimization: On Gradient descent solving Quadratic problems—Guest lecture of MARO 201—Advanced Optimization. https://angms.science/doc/teaching/GDLS.pdf

Ang, A. (2020b). Nonnegative Least Squares—PGD, accelerated PGD and with restarts. https://angms.science/doc/NMF/nnls_pgd.pdf

Denison, R. N., Parker, J. A., & Carrasco, M. (2020). Modeling pupil responses to rapid sequential events. Behavior Research Methods, 52(5), 1991–2007. https://doi.org/10.3758/s13428-020-01368-6

Fink, L., Simola, J., Tavano, A., Lange, E. B., Wallot, S., & Laeng, B. (2021). From pre-processing to advanced dynamic modeling of pupil data. PsyArXiv. https://doi.org/10.31234/osf.io/wqvue

Hoeks, B., & Levelt, W. (1993). Pupillary dilation as a measure of attention: A quantitative system analysis. Behav. Res. Meth. Ins. C., 25, 16–26.

Slawski, M., & Hein, M. (2014). Non-negative least squares for high-dimensional linear models: Consistency and sparse recovery without regularization. ArXiv:1205.0953 [Math, Stat]. http://arxiv.org/abs/1205.0953

Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013). On the importance of initialization and momentum in deep learning. Proceedings of the 30th International Conference on Machine Learning, 1139–1147. https://proceedings.mlr.press/v28/sutskever13.html

Wierda, S. M., van Rijn, H., Taatgen, N. A., & Martens, S. (2012). Pupil dilation deconvolution reveals the dynamics of attention at high temporal resolution. Proceedings of the National Academy of Sciences of the United States of America, 109(22), 8456–8460. https://doi.org/10.1073/pnas.1201858109

Wood, S. N., & Fasiolo, M. (2017). A generalized Fellner-Schall method for smoothing parameter optimization with application to Tweedie location, scale and shape models. Biometrics, 73(4), 1071–1081. https://doi.org/10.1111/biom.12666

Wood, S. N. (2017). Generalized Additive Models: An Introduction with R, Second Edition (2nd ed.). Chapman and Hall/CRC.
