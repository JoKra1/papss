# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' Penalty abstract class (interface)
#' Is implemented by a specific penalty (probably first identity).
#' Needs a public method to get the raw penalty matrix and the same matrix
#' parameterized with a lambda term.
#' These penalty terms are discussed extensively in Wood (2017)
#' 'Generalized Additive Models : An introduction with R, Second Edition'
NULL

#' Dimension getter
NULL

#' Identity penalty class implements Penalty interface.
NULL

#' Constructor for IdentityPenalty.
NULL

#' Returns identity penalty of dimensions (dim, dim)
NULL

#' Returns a lambda parameterized penalty of dimensions (dim, dim)
NULL

#' LambdaTerm class. As discussed in Wood (2017), multiple penalties (for individual
#' terms) can share the same lambda value (e.g., this is achieved with the 'id' keyword
#' in mgcv, see link below).
#' Thus, the LambdaTerm class stores smart pointers to all the penalties
#' associated with this lambda value. It comes with a method to extract these penalties
#' (Not sure whether that is needed, just thought it might be handy..), a method to embedd these
#' penalties in a zero-padded matrix (S in Wood 2017) to represent them in quadratic form cf' * S * cf
#' where cf contains a weight for each column in the model matrix (Wood, 2017 s. 4.3.1).
#' This same method can also be used to embed multiple lambda terms in the same zero-padded matrix, which is
#' required for the generalized Fellner Schall update described in Wood & Fasiolo (2017):
#' This update is also implemented as a method here so that the lambda values of each term can be updated.
#'
#' See: 'id' at https://www.rdocumentation.org/packages/mgcv/versions/1.8-38/topics/s
NULL

#' Associate n penalties of dimension (dim) with this LambdaTerm.
NULL

#' Return a const reference to the penalties. Method is itself declared as const
#' since it should never modfiy an instance which calls this.
#' See: https://isocpp.org/wiki/faq/const-correctness
NULL

#' Embed all penalties belonging to this term in a zero-padded matrix embS as discussed by Wood (2017, s. 4.3.1).
#' cIndex corresponds to the starting index (row & column) at which we want to start embedding the penalties.
#' This is handy in case the model matrix contains un-penalized terms. Thus, any model matrix used by this implementation
#' should be ordered - starting with unpenalized columns/terms following by penalized terms.
NULL

#' Perform a generalized Fellner Schall update step for a lambda term. This update rule is
#' discussed in Wood & Fasiolo (2017). In the paper, the authors provide the update in terms
#' of X and y (as well as X and z for generalized models). However, as discussed in Wood (2017)
#' and Wood (2011): the explicit calculation of (X' * X + embS)^-1 is undesirable.
#' Thus we here invoke the update on 'updated terms' (see code below for a quick overview
#' and Wood, 2011; Wood, 2017 for more details) obtained after repeated QR factorization
#' and Cholesky decomposition, as described extensively in Wood (2011, 2017) to improve on
#' the ill-conditioned nature of the former term.
NULL

#' Enforces positivity constraints on a vector. Based on the work by Hoeks & Levelt (1993)
#' we require all 'attention spikes', i.e., the weights in our cf vector to be positive.
#' Thus, we unfortunately cannot rely on a closed solution for our optimization problem but have to
#' resort to perform projected gradient optimization, as discussed by Ang (2020a; 2020b)
NULL

#' Gradient descent optimizer with momentum and restarts. The momentum update
#' rule is the one discussed by Sutskever et al. (2013) that is also discussed (in slightly
#' alternated form) in the lecture series by Ang (2020). Allows for a projection step
#' to solve constrained optimization problems (e.g., Non-negative least squares [NNLS] -
#' see Bolduc et al. (2017) or Ang (2020a; 2020b)). Further permits for optimizing a
#' penalized NNLS in case embS is different from a zero matrix.
#'
#' For discussion of why momentum helps/matters and the exact momentum rule used here
#' see Sutskever et al. (2013). The algorithm and code itself is based on the
#' pseudo-code from the slides in the lecture series by Ang (2020) and has been
#' adapted to solve a penalized NNLS instead of a simple NNLS. The gradient of the
#' penalized least squares loss function is from Wood (2017).
NULL

#' Fits an additive model based on the stable LS solutions discussed in Wood (2011,2017).
#' @export
solveAM <- function(X, y, initCf, constraints, lambdaTermFreq, lambdaTermDim, startIndex, maxIter, maxIterOptim, tol = 0.001) {
    .Call('_papss_solveAM', PACKAGE = 'papss', X, y, initCf, constraints, lambdaTermFreq, lambdaTermDim, startIndex, maxIter, maxIterOptim, tol)
}

